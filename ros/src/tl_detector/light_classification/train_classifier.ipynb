{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import random \n",
    "from tensorflow.contrib.layers import flatten\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size = (32,32)\n",
    "batch_size = 32\n",
    "epochs = 14\n",
    "learning_rate = .0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "green_images = glob.glob(\"image_data/green/*.png\")\n",
    "yellow_images = glob.glob(\"image_data/yellow/*.png\")\n",
    "red_images = glob.glob(\"image_data/red/*.png\")\n",
    "unknown_images = glob.glob(\"image_data/unknown/*.png\")\n",
    "\n",
    "labels = [\"green\"] * len(green_images) + ['yellow'] * len(yellow_images) + ['red'] * len(red_images) + ['unknown'] * len(unknown_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data for test, train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traffic_light_data = pd.DataFrame({\"image_path\": green_images + yellow_images + red_images + unknown_images,\n",
    "                                  \"labels\": labels})\n",
    "\n",
    "# shuffle df\n",
    "traffic_light_data = traffic_light_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size = int(len(traffic_light_data)  * .25)\n",
    "validation_size = int(len(traffic_light_data) * .0825)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_set = traffic_light_data.loc[:test_size, ]\n",
    "validation_set = traffic_light_data.loc[test_size:(test_size + validation_size), ]\n",
    "training_set = traffic_light_data.loc[test_size + validation_size:, ]\n",
    "\n",
    "print(\"test set size:\", test_set.shape[0])\n",
    "print(\"validation set size:\", validation_set.shape[0])\n",
    "print(\"training set size:\", training_set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create training set\n",
    "X_train = []\n",
    "for i in training_set['image_path']:\n",
    "    image = plt.imread(i)\n",
    "    X_train.append(cv2.resize(image, img_size))\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = training_set['labels']\n",
    "\n",
    "# create validation set\n",
    "X_valid = []\n",
    "for i in validation_set['image_path']:\n",
    "    image = plt.imread(i)\n",
    "    X_valid.append(cv2.resize(image, img_size))\n",
    "X_valid = np.array(X_valid)\n",
    "y_valid = validation_set['labels']\n",
    "    \n",
    "# create test set\n",
    "X_test = []\n",
    "for i in test_set['image_path']:\n",
    "    image = plt.imread(i)\n",
    "    X_test.append(cv2.resize(image, img_size))\n",
    "X_test = np.array(X_test)\n",
    "y_test = test_set['labels']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_img(img):\n",
    "    h,w,c = img.shape \n",
    "    \n",
    "    # normalize\n",
    "    img = cv2.normalize(img , img , alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        \n",
    "    # add rotation\n",
    "    rotattion_angle = random.uniform(-5,5)\n",
    "    M = cv2.getRotationMatrix2D((h/2,w/2), rotattion_angle, 1)\n",
    "    img = cv2.warpAffine(img,M,(w,h))\n",
    "\n",
    "    # flip\n",
    "    if random.uniform(0,1) > .5:\n",
    "        img =  cv2.flip(img, 1 )\n",
    "\n",
    "    # color jitter\n",
    "    noise_g = np.random.uniform(-.05, .05, (h, w))\n",
    "    noise_r = np.random.uniform(-.05, .05, (h, w))\n",
    "    noise_b = np.random.uniform(-.05, .05, (h, w))\n",
    "    \n",
    "    zitter = np.zeros_like(img)\n",
    "    zitter[:,:,0] = noise_r\n",
    "    zitter[:,:,1] = noise_g\n",
    "    zitter[:,:,2] = noise_b\n",
    "    \n",
    "    noise_added = cv2.add(img, zitter)  \n",
    "    \n",
    "    # cap back to [0,1]\n",
    "    noise_added[noise_added > 1] = 1\n",
    "    noise_added[noise_added < 0] = 0\n",
    "       \n",
    "    return noise_added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_image = X_train[random.sample(range(0,len(X_train)),1)]\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs = axs.ravel()\n",
    "index = random.randint(0, len(X_train))\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('before')\n",
    "axs[0].imshow(random_image[0])\n",
    "\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('after')\n",
    "axs[1].imshow(read_and_process_img(random_image[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2 = 0\n",
    "conv1 = 0\n",
    "def LeNet(x, keep_prob):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    global conv1\n",
    "    global conv2\n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    #dropout\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    #dropout\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "logits = LeNet(x, keep_prob)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "# Model Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "# # Train the model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    print()\n",
    "    for i in range(epochs):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        # apply deformation to image to augment\n",
    "        X_train = [process_img(x) for x in X_train]\n",
    "        for offset in range(0, num_examples, batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.50})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, 'traffic_signal')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stuff used for behavioural cloning, generator to make GPU happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_image(image_list):\n",
    "    \"\"\"\n",
    "    Generator function\n",
    "    Generator function, saves RAMs by generating data, instead of pushing it all into your memory in one go\n",
    "    :param image_list:\n",
    "    :return: images and angles\n",
    "    \"\"\"\n",
    "    ii = 0\n",
    "    while True:\n",
    "        images_out = np.ndarray(shape=(batch_size, img_size[0], img_size[1], 3), dtype=float)\n",
    "        labels_out = np.ndarray(shape=batch_size, dtype=float)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            if ii > batch_size:\n",
    "                image_list = shuffledf(image_list)\n",
    "                ii = 0\n",
    "\n",
    "            labels_out[j] = image_list[ii][1]\n",
    "            images_out[j] = read_and_process_img(image_list[ii][0])\n",
    "\n",
    "            ii += 1\n",
    "\n",
    "\n",
    "        yield images_out, labels_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_samples_per_epoch(array_size, batch_size):\n",
    "    \"\"\"\n",
    "    Calculates sample per epoc,\n",
    "    :param array_size: length of the training set (or training, validation set)\n",
    "    :param batch_size: Batch size\n",
    "    :return: Sample size for each epoch\n",
    "    \"\"\"\n",
    "    num_batches = array_size / batch_size\n",
    "    samples_per_epoch = math.ceil((num_batches / batch_size) * batch_size)\n",
    "    samples_per_epoch = samples_per_epoch * batch_size\n",
    "    return samples_per_epoch"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
